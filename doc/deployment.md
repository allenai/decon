# Decon Deployment Guide

This guide covers deploying Decon on EC2 clusters using the poormanray (pmr) tool.

It covers the high level design to understand the moving parts and includes tips for running lots of datasets through decon quickly.

## Prerequisites

1. **Install poormanray**: Follow the instructions at [Poor Man's Ray CLI](https://github.com/allenai/olmo-cookbook/blob/main/README.md#poor-mans-ray-cli)
  - **AWS Credentials**: Ensure your AWS credentials are configured
  - **SSH Key**: Have an SSH key ready (default: `~/.ssh/id_rsa`)

2. **GitHub Token**: The decon repository is private, you'll need a GitHub personal access token for the clone command in decon setup.

## Overview

### Poormanray

Poormanray is a tool for running commands across a group of EC2 instances.

To run decon on a single dataset there are 4 steps.

1. Create the cluster
2. Run decon setup to install software
3. Launch a decon server
4. Launch an orchestrator to download files, pass to the server, and upload results

The poormanray commands to run this for a single dataset can be generated by running following command and providing config when prompted. Sensible defaults are offered and should be accepted when in doubt. Care should be taken to ensure the output paths are correct when submitting new orchestrator runs to avoid overwriting files.

```bash
make poormanray-command-generator
```

Example outputs for a dataset are shown below to inform the later discussion.

#### Creating a cluster

```
poormanray create \
  --name decon \
  --owner robert \
  --number 12 \
  --instance-type i4i.4xlarge \
  --region us-east-1
```

#### Set up decon

```
poormanray setup-decon \
  --name decon \
  --ssh-key-path ~/.ssh/id_rsa \
  --github-token xxxx
```

#### Launch server

```
poormanray run \
  --name decon \
  --command "cd decon && nohup cargo run --release -- server \
    --config config/default.yaml \
    --content-key text \
    --question-threshold 0.815 \
    --answer-threshold 0.8 \
    --reference-input fixtures/reference \
    --report-output-dir /mnt/decon-work/results \
    --cleaned-output-dir /mnt/decon-work/cleaned \
    --purify \
    > server.log 2>&1 & disown" \
  --ssh-key-path ~/.ssh/id_rsa \
  --detach
```

#### Run an orchestrator to process a dataset

```
poormanray run \
  --name decon \
  --command "cd decon && nohup python python/orchestration.py \
    --config config/orchestration.yaml \
    --remote-file-input s3://ai2-llm/pretraining-data/sources/thinking-data/llama-nemotron-processed-chinese-filtered-ngram-filtered-with-token-counts/ \
    --remote-report-output-dir s3://ai2-decon-reports/8-1/ai2-llm-pretraining-data-sources-thinking-data-llama-nemotron-processed-chinese-filtered-ngram-filtered-with-token-counts \
    --remote-cleaned-output-dir s3://ai2-llm/pretraining-data/sources/thinking-data/llama-nemotron-processed-chinese-filtered-ngram-filtered-with-token-counts-decon-2 \
    --local-work-dir /mnt/decon-work \
    > orchestrator.log 2>&1 & disown" \
  --ssh-key-path ~/.ssh/id_rsa \
  --detach
```

### How it works

Decon can be run as a standalone cli invocation **or** as a server. An orchestrator script is provided which downloads source dataset files, passes them to the decon server, and uploads the results.

In this guide we discuss running decon as a server and using the orchestrator on poormanray to decon datasets.

#### Overview

```
┌─────────────────┐
│   S3 Input      │
│ Training Data   │
│ (JSONL files)   │
└────────┬────────┘
         │
         │ Download
         ▼
┌─────────────────────────────────────────────────────────────┐
│                      EC2 Instance(s)                        │
│                                                             │
│  ┌──────────────┐          ┌───────────────────────────┐    │
│  │ Orchestrator │ ────────▶│        Server             │    │
│  │              │  Submit  │                           │    │
│  │ • Downloads  │  Jobs    │  • Loads reference data   │    │
│  │ • Distributes│          │  • Processes files        │    │
│  │ • Uploads    │◀──────── │  • Detects contamination  │    │
│  │              │  Results │  • Creates cleaned files  │    │
│  └──────┬───────┘          └───────────────────────────┘    │
│         │                                                   │
└─────────┼───────────────────────────────────────────────────┘
          │-------------------------
          │ Upload                 | Upload
          ▼                        ▼
    ┌─────────────┐       ┌──────────────────┐
    │ S3 Reports  │       │ S3 Cleaned Files │
    │   Output    │       │    (Optional)    │
    │ (JSONL)     │       │ (JSONL.gz)       │
    └─────────────┘       └──────────────────┘
```

#### Server

When run as a server, a small http server is started. It receives posts that include information about an input file on the local filesystem and produces a report file and optionally a cleaned file with contaminated lines removed.

The server is launched with a specific configuration which is based on a configuration file passed through the --config flag, but with the ability to override configuration options in the config file with various command line options.

Note that the config/default.yaml configuration is the baseline and should be used until you are ready to intentionally run decon with different parameters.

A single server instance runs with this configuration until killed, so if you have multiple datasets to decon with the same configuration you can use a single server instance. Orchestrator's are scoped so it is designed to run multiple orchestrators on different datasets on the same cluster using the same server processes.

#### Orchestrator

The orchestrator is launched with input and output targets on s3.

It maintains a download queue, and monitors the server's work queue creating a pipeline for downloading and processing files.

It also monitors job status reported by the server, and upon completion uploads the output files to the output s3 targets, and cleans up local files after the upload is complete.

Orchestrators are aware of the number of hosts in a cluster and the current host's index through environment variable set when running the decon setup script on poormanray. This is used to partition the work by s3 file, and only files that hash modulo to an orchestrator's host index will processed by a given orchestrator.

Orchestrators produce their own scoped workspace and only process output files based on the job ids they receive when submitting jobs to the server. This means it is perfectly fine to run many orchestrators, one per dataset being decontaminated, at the same time. Just remember that a server process only has one set of decontamination parameters.

### Practical Tips

You can anticipate that a single i4i.4xlarge host can process 26B tokens/hour.

Work is distributed by s3 file hash so it's possible to have uneven workloads when running small datasets on larger clusters.

There is no great way to see when work is complete, this is a TODO, but the cumbersome way to do it today is to check for orchestrator lock files. An orchestrator creates a lock file which effectively limits one orchestrator per input source hash per host, in /mnt/decon-work/<workspace-hash>. The lock file is removed when work is complete, and all hosts can be checked with `poormanray run -n <your-cluster-name> -c "ls -R /mnt/decon-work | grep lock"` and observing no active lock files.

Because it can take a while to launch a large cluster, when processing datasets with different contamination parameters, it can be nice to restart the server with a new config. The server responds to SIGTERM by shutting down, so the servers can be stopped with `poormanray run -n <your-cluster-name> -c "pkill decon"`. At which point a new server launch command can be run on the cluster, with alternative configuration override options set.

The orchestrator will poll for a while when first launched in case the server is still processing the eval index (takes ~ 1 minute) so you can launch orchestrators quickly after launching servers.

In general the i4i.4xlarge is wildly over-provisioned. This is done partly just because the purification step reads files directly into memory before writing clean versions and I've observed datasets with large files, if 10 7GB files are being purified at the same time that is a lot of RAM. This is a TODO to make this more efficient, but you can use instances with more CPU if you know the shape of your dataset. And can be confident it's fine to run MANY orchestrators at the same time, e.g. processing a full training dataset at once.

When processing a lot of datasets, I update the generate_orchestrator_command.py file with my cluster and output base options and run `make generate-orchestrator-command s3://new-input-prefix` to quickly generate orchestrator commands. This is again not very polished, and it's fine to manually update a first orchestrator launch command generated from `make poormanray-command-generator`.

